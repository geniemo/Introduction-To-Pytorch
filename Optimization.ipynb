{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "babb5e18",
   "metadata": {},
   "source": [
    "# Optimizing Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ae5e0",
   "metadata": {},
   "source": [
    "모델을 학습시키는 것은 반복적 과정이다. 각 반복에서 모델은 예측을 하고, 이 예측에 대한 loss를 계산하고, derivative를 수집한 뒤 gradient descent를 이용해 parameter를 최적화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e05932",
   "metadata": {},
   "source": [
    "## Prerequisite Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dcb7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d50b3",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed67636b",
   "metadata": {},
   "source": [
    "Hyperparameters는 adjustable parameters로 최적화 과정에서 제어할 수 있는 것들이다. 하이퍼 파라미터 튜닝에 대해 더 알아보려면 [여기](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)를 살펴보자.\n",
    "\n",
    "튜닝을 위해 다음과 같은 하이퍼파리미터를 정의한다.\n",
    "- 에폭 수\n",
    "- 배치 사이즈\n",
    "- 학습률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6923846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047e728",
   "metadata": {},
   "source": [
    "## Optimization Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7682568",
   "metadata": {},
   "source": [
    "하이퍼파라미터를 설정한 후에는 최적화 단계를 통해 모델을 학습하고 최적화할 수 있다. 이 때 각 반복을 에폭이라고 한다.\n",
    "\n",
    "한 에폭은 다음과 같이 구성된다.\n",
    "- The Train Loop - 전 학습 데이터 셋을 iterate하고 최적 파라미터로 수렴하려고 한다.\n",
    "- The Validation/Test Loop - 전 테스트 데이터 셋을 iterate하고 모델 성능이 향상되고 있는지 확인한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4d086",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e614d7be",
   "metadata": {},
   "source": [
    "손실 함수에는 다음과 같은 것들이 있다.\n",
    "- nn.MSELoss(Mean Square Error)\n",
    "- nn.NLLLoss(Negative Log Likelihood)\n",
    "- nn.CrossEntropyLoss(nn.LogSoftmax와 nn.NLLLoss를 합친 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d891fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64edf4d2",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5078a4d8",
   "metadata": {},
   "source": [
    "SGD, ADAM, RMSProp 등 다양한 optimizer들을 [여기](https://pytorch.org/docs/stable/optim.html)서 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdec47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba460603",
   "metadata": {},
   "source": [
    "training loop에서 최적화는 다음과 같은 과정으로 이루어진다.\n",
    "- optimizer.zero_grad()로 모델 파라미터의 gradients를 초기화.\n",
    "- loss.backward()로 backpropagate\n",
    "- gradients를 통해 parameters를 조절하도록 optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0fb32",
   "metadata": {},
   "source": [
    "## Full Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c5ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    # Set the model training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practice\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3347475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.931778  [   64/60000]\n",
      "loss: 1.905345  [ 6464/60000]\n",
      "loss: 1.798592  [12864/60000]\n",
      "loss: 1.834560  [19264/60000]\n",
      "loss: 1.708631  [25664/60000]\n",
      "loss: 1.680793  [32064/60000]\n",
      "loss: 1.679808  [38464/60000]\n",
      "loss: 1.595789  [44864/60000]\n",
      "loss: 1.606664  [51264/60000]\n",
      "loss: 1.503358  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Avg loss: 1.523480 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.586879  [   64/60000]\n",
      "loss: 1.557206  [ 6464/60000]\n",
      "loss: 1.411592  [12864/60000]\n",
      "loss: 1.482002  [19264/60000]\n",
      "loss: 1.346609  [25664/60000]\n",
      "loss: 1.359735  [32064/60000]\n",
      "loss: 1.357467  [38464/60000]\n",
      "loss: 1.294370  [44864/60000]\n",
      "loss: 1.321269  [51264/60000]\n",
      "loss: 1.226008  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.252660 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.330443  [   64/60000]\n",
      "loss: 1.314949  [ 6464/60000]\n",
      "loss: 1.151062  [12864/60000]\n",
      "loss: 1.258128  [19264/60000]\n",
      "loss: 1.121972  [25664/60000]\n",
      "loss: 1.160143  [32064/60000]\n",
      "loss: 1.172444  [38464/60000]\n",
      "loss: 1.116488  [44864/60000]\n",
      "loss: 1.150190  [51264/60000]\n",
      "loss: 1.073091  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.090952 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.164756  [   64/60000]\n",
      "loss: 1.167325  [ 6464/60000]\n",
      "loss: 0.984518  [12864/60000]\n",
      "loss: 1.122377  [19264/60000]\n",
      "loss: 0.987988  [25664/60000]\n",
      "loss: 1.029112  [32064/60000]\n",
      "loss: 1.062692  [38464/60000]\n",
      "loss: 1.005985  [44864/60000]\n",
      "loss: 1.039761  [51264/60000]\n",
      "loss: 0.979711  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.987707 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.049912  [   64/60000]\n",
      "loss: 1.072484  [ 6464/60000]\n",
      "loss: 0.870938  [12864/60000]\n",
      "loss: 1.032360  [19264/60000]\n",
      "loss: 0.904169  [25664/60000]\n",
      "loss: 0.936239  [32064/60000]\n",
      "loss: 0.991483  [38464/60000]\n",
      "loss: 0.934059  [44864/60000]\n",
      "loss: 0.962213  [51264/60000]\n",
      "loss: 0.916260  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.916218 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.963888  [   64/60000]\n",
      "loss: 1.005433  [ 6464/60000]\n",
      "loss: 0.788385  [12864/60000]\n",
      "loss: 0.967264  [19264/60000]\n",
      "loss: 0.847108  [25664/60000]\n",
      "loss: 0.866651  [32064/60000]\n",
      "loss: 0.940665  [38464/60000]\n",
      "loss: 0.885205  [44864/60000]\n",
      "loss: 0.904656  [51264/60000]\n",
      "loss: 0.869395  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.863425 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.896333  [   64/60000]\n",
      "loss: 0.953916  [ 6464/60000]\n",
      "loss: 0.725526  [12864/60000]\n",
      "loss: 0.917238  [19264/60000]\n",
      "loss: 0.805316  [25664/60000]\n",
      "loss: 0.813183  [32064/60000]\n",
      "loss: 0.901495  [38464/60000]\n",
      "loss: 0.850725  [44864/60000]\n",
      "loss: 0.860473  [51264/60000]\n",
      "loss: 0.832437  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.822554 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.841259  [   64/60000]\n",
      "loss: 0.911694  [ 6464/60000]\n",
      "loss: 0.676274  [12864/60000]\n",
      "loss: 0.877336  [19264/60000]\n",
      "loss: 0.772607  [25664/60000]\n",
      "loss: 0.771364  [32064/60000]\n",
      "loss: 0.869043  [38464/60000]\n",
      "loss: 0.825244  [44864/60000]\n",
      "loss: 0.825733  [51264/60000]\n",
      "loss: 0.801904  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.789617 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.795072  [   64/60000]\n",
      "loss: 0.875444  [ 6464/60000]\n",
      "loss: 0.636680  [12864/60000]\n",
      "loss: 0.844991  [19264/60000]\n",
      "loss: 0.745701  [25664/60000]\n",
      "loss: 0.737988  [32064/60000]\n",
      "loss: 0.840574  [38464/60000]\n",
      "loss: 0.805343  [44864/60000]\n",
      "loss: 0.797518  [51264/60000]\n",
      "loss: 0.775746  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.762147 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.755646  [   64/60000]\n",
      "loss: 0.843319  [ 6464/60000]\n",
      "loss: 0.603947  [12864/60000]\n",
      "loss: 0.818093  [19264/60000]\n",
      "loss: 0.722828  [25664/60000]\n",
      "loss: 0.711220  [32064/60000]\n",
      "loss: 0.815061  [38464/60000]\n",
      "loss: 0.788922  [44864/60000]\n",
      "loss: 0.773848  [51264/60000]\n",
      "loss: 0.752828  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.738487 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
