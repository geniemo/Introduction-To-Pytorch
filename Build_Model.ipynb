{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc4b9ec",
   "metadata": {},
   "source": [
    "# Build The Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d826ae6",
   "metadata": {},
   "source": [
    "torch.nn namespace는 신경망을 구성하는 데 필요한 모든 구성 요소를 제공한다. PyTorch의 모든 모듈은 nn.Module의 subclass이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8ee8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddaa52e",
   "metadata": {},
   "source": [
    "## Get Device for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12455176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a762b",
   "metadata": {},
   "source": [
    "## Define the Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f480965",
   "metadata": {},
   "source": [
    "Neural network class는 nn.Module의 subclass로 만든다. 그리고 layers를 \\_\\_init\\_\\_에서 초기화한다. nn.Module의 subclass는 input data에 대한 처리를 forward 메서드에서 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84967eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 18),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc292b0",
   "metadata": {},
   "source": [
    "NeuralNetwork 객체를 만들고 device로 옮긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c0d5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=18, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c3f83",
   "metadata": {},
   "source": [
    "model을 사용할 때에는 forward를 직접 호출하는 대신 model에 데이터를 전달한다. 이렇게 하면 일부 백그라운드 연산들과 함께 모델의 forward를 실행한다.\n",
    "\n",
    "model에 데이터를 전달하면 2차원 텐서를 반환한다. dim=0은 각 분류(class)에 대한 raw predicted values가, dim=1에는 각 출력의 개별적인 값들이 있다.\n",
    "\n",
    "아래와 같이 이 값을 nn.Softmax에 넘겨주어 확률값을 얻어낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af77f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0389, -0.0472, -0.0112,  0.0729, -0.1464, -0.0405, -0.0185, -0.0261,\n",
       "          -0.0052, -0.0878, -0.0349,  0.0007,  0.0522, -0.0990,  0.0286,  0.0696,\n",
       "          -0.0225, -0.0342],\n",
       "         [ 0.0339, -0.0398, -0.0264,  0.0665, -0.1718,  0.0506, -0.0013, -0.0844,\n",
       "           0.0395, -0.0820,  0.0079, -0.0066,  0.0618, -0.1236, -0.0275,  0.1001,\n",
       "          -0.0152, -0.0486],\n",
       "         [ 0.0561, -0.0814, -0.0153,  0.0646, -0.1164,  0.0562,  0.0037, -0.0288,\n",
       "          -0.0017, -0.1036,  0.0489,  0.0403,  0.0168, -0.0820, -0.0051,  0.0227,\n",
       "          -0.0064, -0.0506]], grad_fn=<AddmmBackward0>),\n",
       " torch.Size([3, 18]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(3, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "logits, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c185d6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([ 3, 15,  3])\n"
     ]
    }
   ],
   "source": [
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f'Predicted class: {y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c89c5f",
   "metadata": {},
   "source": [
    "## Model Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb4c09",
   "metadata": {},
   "source": [
    "FashionMNIST 모델의 layer들을 자세히 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "539bcffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "input_image.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79b74ad",
   "metadata": {},
   "source": [
    "nn.Flatten은 28x28 image를 784개의 픽셀 값들을 갖고 있는 contiguous array로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdf66f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 784])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "flat_image.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05dafc",
   "metadata": {},
   "source": [
    "nn.Linear은 저장된 weights와 bias를 사용해 linear transformation을 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ec32f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28 * 28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "hidden1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea5f7e",
   "metadata": {},
   "source": [
    "이 모델에서는 nn.ReLU를 Non-linear 활성화를 위해 사용하는데, 다른 활성화를 도입할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cd31804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.3632,  0.2206, -0.0947, -0.0158, -0.0545, -0.0433,  0.1038, -0.2928,\n",
      "          0.1661,  0.0125, -0.2600,  0.2028, -0.4580,  0.5020,  0.2187,  0.1670,\n",
      "         -0.0795,  0.1862,  0.1253,  0.1903],\n",
      "        [-0.4817, -0.0630, -0.1336,  0.1391, -0.3439, -0.2601, -0.0034, -0.1140,\n",
      "          0.1843, -0.1564, -0.0600,  0.0504, -0.3888,  0.4677,  0.2818, -0.1439,\n",
      "          0.0840,  0.3408,  0.4783, -0.0702],\n",
      "        [-0.4678,  0.0444,  0.2156,  0.0777, -0.3044, -0.1166,  0.0472, -0.2092,\n",
      "          0.0175,  0.2032, -0.2841, -0.0456, -0.2136,  0.3230,  0.2733,  0.1156,\n",
      "          0.0054,  0.1817,  0.2453,  0.1578]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.2206, 0.0000, 0.0000, 0.0000, 0.0000, 0.1038, 0.0000, 0.1661,\n",
      "         0.0125, 0.0000, 0.2028, 0.0000, 0.5020, 0.2187, 0.1670, 0.0000, 0.1862,\n",
      "         0.1253, 0.1903],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1391, 0.0000, 0.0000, 0.0000, 0.0000, 0.1843,\n",
      "         0.0000, 0.0000, 0.0504, 0.0000, 0.4677, 0.2818, 0.0000, 0.0840, 0.3408,\n",
      "         0.4783, 0.0000],\n",
      "        [0.0000, 0.0444, 0.2156, 0.0777, 0.0000, 0.0000, 0.0472, 0.0000, 0.0175,\n",
      "         0.2032, 0.0000, 0.0000, 0.0000, 0.3230, 0.2733, 0.1156, 0.0054, 0.1817,\n",
      "         0.2453, 0.1578]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Before ReLU: {hidden1}\\n\\n')\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f'After ReLU: {hidden1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a4f85",
   "metadata": {},
   "source": [
    "nn.Sequential은 순서를 갖는 모듈의 컨테이너이다. 데이터는 미리 정해진 순서대로 모듈들에 전달된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2518a455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0366,  0.1074,  0.2082, -0.0566, -0.1334, -0.1278, -0.0894,  0.1909,\n",
       "          0.3720,  0.0464],\n",
       "        [ 0.0390,  0.1764,  0.1176, -0.0997, -0.1956, -0.0984, -0.0989,  0.1648,\n",
       "          0.2692,  0.0159],\n",
       "        [ 0.1136,  0.0981,  0.2394, -0.0691, -0.1553, -0.1042, -0.1500,  0.1729,\n",
       "          0.2494,  0.0121]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb373560",
   "metadata": {},
   "source": [
    "nn.Softmax를 통해 [-infty, infty] 범위의 logits를 [0, 1] 범위로 scaling해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17eba1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0969, 0.1040, 0.1150, 0.0883, 0.0817, 0.0822, 0.0854, 0.1131, 0.1355,\n",
       "         0.0978],\n",
       "        [0.1000, 0.1147, 0.1081, 0.0870, 0.0791, 0.0871, 0.0871, 0.1134, 0.1258,\n",
       "         0.0977],\n",
       "        [0.1064, 0.1048, 0.1207, 0.0886, 0.0813, 0.0856, 0.0817, 0.1129, 0.1219,\n",
       "         0.0961]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "pred_probab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61639af",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e311a9d5",
   "metadata": {},
   "source": [
    "Neural network 안의 layer들은 parameterize 된다.(즉, weights와 biases가 training 중 최적화된다.)\n",
    "\n",
    "nn.Module을 상속하면 자동적으로 모델 객체 내의 모든 필드를 추적하고, 이는 parameters()나 named_parameters()를 통해 모든 파라미터에 접근 가능하게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "150d40e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=18, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values: tensor([[ 0.0225, -0.0315, -0.0043,  ...,  0.0202, -0.0245, -0.0123],\n",
      "        [-0.0168, -0.0316, -0.0354,  ..., -0.0278, -0.0066,  0.0245]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values: tensor([-0.0043,  0.0037], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values: tensor([[ 0.0147,  0.0035, -0.0432,  ...,  0.0256, -0.0109,  0.0319],\n",
      "        [ 0.0285, -0.0317,  0.0194,  ..., -0.0269,  0.0132, -0.0304]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values: tensor([-0.0310, -0.0066], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([18, 512]) | Values: tensor([[-0.0417, -0.0428,  0.0115,  ...,  0.0001, -0.0070,  0.0413],\n",
      "        [ 0.0263,  0.0200,  0.0339,  ...,  0.0199,  0.0293,  0.0207]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([18]) | Values: tensor([ 0.0405, -0.0159], grad_fn=<SliceBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Model Structure: {model}\\n\\n')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'Layer: {name} | Size: {param.size()} | Values: {param[:2]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
